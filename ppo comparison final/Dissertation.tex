\documentclass[11pt]{article}

\usepackage{titling}
\usepackage{blindtext}
\usepackage{url}
\usepackage{amsmath, amssymb}
\usepackage{booktabs} 
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm, marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Embedding Domain-Specific Knowledge through Reward Shaping in Reinforcement Learning \\[1ex] \large Dissertation}

\author{Vignesh Mohanarajan}

\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}
\tableofcontents
\pagebreak

\section{Introduction}

The aim of my project is to demonstrate the impact of real-world domain-specific knowledge on the training of reinforcement learning (RL) models operating on simulations of real world environments. This is done in the Google Research Football (GFootball) environment \cite{Kurach2020} with various reinforcement learning algorithms and the use of advanced football metrics in a novel reward shaping method. The reward given by the environment is an essential component of an RL algorithm's learning process, as it provides the feedback necessary for the agent to adapt its strategies and behaviors over time. Thus, adjusting the reward in an effective manner should fundamentally impact the performance of a learning algorithm. While this is done in an environment that simulates football, we believe that similar incorporation of domain knowledge can be used in other environments to improve learning with the potential downside of restricting the model to human knowledge rather than allowing it to expand beyond the current domain knowledge.


\subsection{Reinforcement Learning}
Reinforcement learning (RL) is the process of training a model to return an optimum solution for a problem by taking a sequence of decisions by itself and analysing the reward received for doing so \cite{Wu2022}. In recent years, institutions such as Google DeepMind have targeted a few different environments to provide newer challenges for them to demonstrate the potential of their algorithms and models such as Agent57 \cite{Badia2020}, the first deep RL agent to outperform the standard human benchmark on each of the 57 Atari games and AlphaGo \cite{Silver2016} the first computer program to beat the world champion in the game Go, an abstract strategy board game. The reason for using games such as these is their tendency to possess well defined rules and objectives, making it easier to define a reward function for the agent which provides vital information allowing it to differentiate between a beneficial action and a disadvantageous action in the context of the game.

However, as more advancements are made in the field, the question of whether reinforcement learning could achieve significant results in more challenging environments is raised. As a result, environments such as MuJoCo \cite{MuJoCo} which aims to train RL agents to tackle problems in robotics and bio-mechanics, NeuralMMO \cite{suarez2019neural}, a massive multi-agent game environment, and Starcraft Multi-Agent Challenge (SMAC) \cite{samvelyan2019starcraft} which transforms the computer game StarCraft II into a rich RL environment were created. These environments are much different from the previous ones due to the complexity of the state space and action space. This allows us to challenge our assumptions about the effectiveness and acceptable ranges of the various hyper-parameters of each algorithm and explore potential strengths and weaknesses.

The GFootball environment was created to train agents in a digital adaptation of the game of football. In more complex environments, the reward function could be as simple as a goal in the example of football, the decisive result in StarCraft, or the distance travelled by a bipedal robot in the case of the MuJoCo environment.
However, these rewards could be too simple and not consistent enough, especially in the case of a game such as football where the average number of goals is lower than 3 (in the English Premier League \cite{eplgoals2021}). This real-world behaviour of football translates into the GFootball environment leading to the well-known “sparse-reward” problem , wherein an environment rarely produces a significant reward signal, thereby challenging the agent’s ability to learn. So with environments that are prone to this, how could this problem we tackled?

\subsection{Data Science in Football}
In real-world football, great strides have been made by football clubs to use data science techniques to better understand the game from a statistical perspective. Rather than simpler metrics such as shots and possession, which have been counted since the 1986 World cup, there has been a rise in the use of advanced metrics such as expected goals, pitch control, expected possession value, and expected threat. ’Expected goals’ (xG) \cite{mead2023expected} aims to measure the chance of a shot becoming a goal and thus its ‘quality’. ’Pitch control’ (PC) \cite{Spearman2018} is a probabilistic model of whether team is likely to be in possession of the ball depending on its position on the pitch depending on the location and velocity of each player. 'Expected possession value' (EPV) \cite{Fernandez2021} is a framework that quantifies the outcome of a possession (a phase of play in which a team currently has the ball) into a real number in the [-1, 1] range where a score of -1 would imply that the team currently in possession of the ball are likely to concede a goal based on the current state whereas 1 would imply they are likely to score a goal. Expected Threat (xT) \cite{karun_introducing_2019} quantifies the likelihood of a team scoring a goal in the next $n$ actions based on the ball location into a numerical value typically from 0 to 1. The action is one of a dribble, pass or a shot. Thus the quality of a pass can be roughly quantified by the difference between the xT in the origin area and the xT in the destination. These metrics quantify the quality of any state or action in the game, and potentially solve the aforementioned sparse-reward problem in environments that aim to simulate football, but specifically the GFootball environment.

\subsection{Comparative Study Layout}
The aforementioned aim is thus to be achieved via a comparative study of the performance of various reinforcement learning algorithms with and without the proposed novel reward shaping method. To conduct this study we will require:

\begin{itemize}
\item A GFootball environment-specific implementation for each of the 4 reinforcement learning algorithms to be used in the study, namely Proximal Policy Optimization, Deep Q Network, Individual Proximal Policy Optimization, and Value Decomposition Network. These were chosen to include a value-based and policy based method from the single-agent and multi-agent algorithms, taking full use of the capabilities of the GFootball to allow control of multiple players simultaneously.

\item To build the expected possession value, expected threat and the expected goals models, an understanding of these models as well as immense amounts of labelled event data from football matches will be required. The more diverse the football data i.e from different countries and leagues, the more general it will be, ensuring that our model does not tend toward playing like anyone nation as football styles highly differ between regions.

\item To implement a calculation of the Pitch Control model that is compatible with a simulation rather than a real-world situation. This would require simplifying changes to the parameters.

\item To use the xG, xT, PC, and EPV models, it is vital to transform the information given by the environment about the state to be usable by these models. This would include normalization of the location coordinates of the players on both teams and the ball.

\item The aforementioned models are then utilised by the reward shaper by feeding information gained from the environment to gain contextual information about the game. A weighted average of this information is then used to reward or punish the agent(s).
\end{itemize}

\pagebreak

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{RLFrameworks.png}
    \caption{(Left) General Reinforcement Learning Framework (Right) Reinforcement Learning with Shaped Reward}
    \label{fig:rl-frameworks}
\end{figure}

In this comparative study, for each algorithm we run reinforcement learning with the framework pictured on the left in Figure \ref{fig:rl-frameworks} and then the framework on the right. We aim to keep all other variables the same as a control.
During training, the agent plays against a rule-based bot provided by the environment whose difficulty can be adjusted according to a value between 0 and 0.95. 
The undertaking of each action $A_t$ by the agent is called a step. In the context of the GFootball environment, a 'game' or episode is 3001 steps.
To study the impact of the reward shaping method, analysis is done primarily on the learning curves produced by the respective frameworks. The learning curve is a graph of the performance in each episode versus the number of steps taken. A higher or steeper learning curve would indicate a faster rate of learning. Other statistics will also be considered for a more holistic quantitative and qualitative analysis. 

\section{Reward Shaping}
This section details the process of creating the reward shaping method used in the study. The reward shaper looks to incorporate 4 models, expected threat (xT), expected goals (xG), expected possession value (EPV), and pitch control (PC). For each model, a detailed explanation of its aim, creation, and adjustments made to work with the GFootball environment.

\subsection{Expected Threat}

Expected Threat ($xT$) \cite{karun_introducing_2019} aims to model team behaviour in possession to understand “buildup play”. “Buildup play” refers to the part of play where the team in possession aims to move the ball further up the pitch to get to a goal-scoring position, effectively breaking the opposition team’s first defensive lines. Expected threat was made in recognition of the fact that most statistics, including those aforementioned, failed to fairly quantify a player’s effect in getting the ball to threatening positions.

$xT$ answers the question of what is the probability of scoring in the next $n$ actions from a certain area in the pitch in $n$ ball-moving actions i.e a shot, dribble, or pass. $n$ can be chosen arbitrarily but the $xT$ value tends to converge after 4-5 actions and is thus chosen as 5 for the purposes of this paper.
In the context of the project's reward shaping, this is to be used to reward and punish passes made by the agent appropriately. The reward and punishment are proportional to the $xT$ gained or lost by the pass.

\subsubsection{Creating the $xT$ model}

The first step to creating the model was to gather labelled event data of events that occurred in football matches. This would include the origin and destination coordinates of a pass, the origin of a shot and whether it led to a goal, the player behind the event, the match, and the teams involved. The required complexity of data does lead to scarcity. However databases of events from matches played in England, France, Germany, Italy, Spain and prominent international tournaments have been made available for the purpose of research \cite{wyscout_data_2019}.

In most statistical studies in football, they are focused on certain regions and thus only use one of the datasets, however as the goal for this paper as it pertains to the $xT$ model, we aim to have a more general model and thus concatenate and use all of the data. This leads to a substantial dataset of approximately 440 MB. 

We filter the events to only look at events that 'move' the ball. This would include passes and dribbles. The data does not include dribbles as it is much harder to track. As a result, the final reward shaper only uses $xT$ to evaluate passes. The task of quantifying a dribble or a possession is handled using another component of the reward shaper to be discussed later. We exclude passes that end up outside the pitch as they lead to a loss of possession and wouldn't inform our model. Datasets tend to be imperfect and we noticed that certain entries were incomplete and had to be removed. Luckily only 2 such rows existed, a very insignificant loss of data.

Note that when building these models, we consider the football pitch to be divided into $16 \times 12$ zones (total of 192 zones). The choice of this resolution is seemingly arbitrary but makes intuitive sense and is used by many papers in the area of football analytics \cite{karun_introducing_2019}. It is possible to increase or decrease the number of zones as one would prefer. Each zone can be denoted by $(x, y)$ where $x \in [1, 16]$ and $y \in [1, 12]$.

After this we filter for all the shots in the database, and for all shots that led to a goal. Using these we calculate the probability of an action taken from a zone being a moving action rather than a shot action or simply the move probability. We then calculate the inverse of the former, namely the probability of an action being a shot action rather than a moving action is referred to as the shot probability.
We then calculate the probability of a goal from a zone given that the action taken was a shot, the goal probability.

For each of the 192 zones we create a 2-dimensional Transition Matrix $T_(x,y)$, each element in the matrix stores the probability that given that a moving action is taken the ball moves from $(x, y)$ to the zone denoted by $(i, j)$ where $i$ and $j$ are the row and column indices of element in the matrix. This is calculated by dividing the number of actions moving the ball from $(x, y)$ to $(i, j)$ over all the moving actions undertaken in the zone $(x, y)$.

$xT$ is considered to be the expected 'payoff' of taking an action from an area. This would be the sum of the expected 'payoff' of a shot and the expected 'payoff' of a move.

For each area, we calculate the expected 'payoff' of a shot. i.e $(s_{x, y} \times g_{x, y}$. This is done using the previously calculated shot and goal probability for each zone.

Then we calculate the expected 'payoff' of a move, for a zone $(x, y)$, is the move probability multiplied by the summation of the $xT$ of all the zones times $T_(x,y)$.

Thus, the formula to calculate $xT$ for a zone $(x, y)$ is

\begin{equation}
\mathbf{x T}^n_{x, y}=\left(s_{x, y} \times g_{x, y}\right)+\left(m_{x, y} \times \sum_{z=1}^{16} \sum_{w=1}^{12} T_{(x, y) \rightarrow(z, w)} \mathbf{x T}^{n-1}_{z, w}\right)
\end{equation}

$xT$ is calculated iteratively, at the nth iteration we calculate the probability of scoring from the zone $(x, y)$ after n actions. So in the first iteration, we calculate the probability of scoring from the zone $(x, y)$ after 1 action and we require ${x T}^0_{x, y}$. Since it is 'impossible' to score with 0 actions we initialise the $xT$ matrix at 0 actions with zeros, i.e $\forall (x,y), {x T}^0_{x, y} = 0 $.

We iterate over this 5 times as this is when the model begins to converge, i.e the values do not change significantly with further iterations. And thus we now have an $xT$ model.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\textwidth]{xT-map.png}
    \caption{A visualization of the $xT$ model generated. Darker areas indicate areas with a higher $xT$. Passes from lighter areas to darker areas will be 'rewarded', and vice versa will be 'punished'.}
    \label{fig:xT-map}
\end{figure}

\subsubsection{Using the $xT$ model}
Now, we have a $16 \times 12$ matrix which represents the chance of scoring a goal in the next 5 actions in each zone. When a pass goes from zone $(x, y)$ to a zone $(a, b)$, we can quantify the value of the pass simply by calculating ${x T}_{a, b} - {x T}_{x, y}$. If we simply weight it appropriately to avoid overvaluing passes, which could lead to agents simply passing repeatedly to maximise the shaped reward, we now have an established metric to reward our agents for 'good' and 'bad' passes. The usage of the $xT$ model as such is novel.

\subsection{Expected Goals}

Expected goals \cite{mead2023expected} or $xG$ is a measure of the probability of a goal occurring from a shot. This gives a quantifiable measure of the 'quality' of a shot. It provides valuable context for a shot. In the case of an agent not scoring due to an exceptional save or other such factors, it should follow that the agent still be rewarded so as to incentivise them to try a similar chance again. It may seem intuitive that shots taken from directly in front of the goal and closer are more likely to lead to goals. This may be intuitive, but the quantification provided by the $xG$ model is vital to be able to use this information in the context of machine learning.

There are many ways to express an $xG$ model. They vary from simple linear models which only take into account the location of the shot to neural networks which accept the location of the ball, surrounding players, and the goalkeeper to generate a probability of the shot leading to a goal \cite{deepxG_2019}.

As we are developing this model to be used in reinforcement learning, it is vital that it not take too long to generate the $xG$ value as this could lead to the training to be far slower. Thus, we use a simpler linear model to calculate $xG$ for the purposes of the reward shaper.

\subsubsection{Creating the $xG$ model}
To generate the $xG$ the same data as the earlier $xT$ model. The datasets contain all the required information and a surplus that will need to be filtered out.

From the dataset, we filter out shots also adding a parameter to specify whether the shot led to a goal or not. This is vital as we will need to calculate 

We add to the dataset a parameter $C$ which is simply the distance between the location of the shot and an imaginary horizontal line running through the centre of the field. This enables an easy calculation using Pythagoras' theorem to find the distance from the shot location to the goal. We also find the angle of the shot as depicted in the figure below, where the small circle signifies the location from which the shot is taken.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\textwidth]{xG-params.png}
    \caption{A pictorial representation of the parameters $\theta$, C, Distance, and (x,y)}
    \label{fig:xG-params}
\end{figure}

Again, with our intuition the higher the shot angle, the more likely we are to score a goal, but how do we quantify this relation?
Thus, we build a model that takes into account the x coordinate of the shot ($X$), the aforementioned parameter $C$ the distance ($D$) and angle ($\theta$) of a shot along with other parameters derived from the former.
The derived parameters are $X^2$, $D^2$, $X \times \theta$, and $C^2$. We use these parameters to fit a Generalised Linear Model (GLM). 

A Generalized Linear Model (GLM) is a flexible framework for modeling relationships between a dependent variable and one or more independent variables. Fitting one to our data allows to efficiently calculate the $xG$ by simply summing up the multiples of the co-efficients with the value of the respective parameter to calculate the log odds. To then retrieve the $xG$ value, we use the simple formula: $xG = 1/(1+e^{log-odds})$.

The visualization of the $xG$ model in \ref{fig:xG-map} aligns with what one would expect. Now we have a model capable of rewarding shots regardless of whether they lead to a goal or not but rather on whether they were likely to go in. This should help inform an agent when they make a good decision but had an unfavourable outcome. In reality, a human player would already know what a good shooting decision is, but in the context of reinforcement learning such information needs to be provided lest the agent be misled into believing they have made an unfavourable decision.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\textwidth]{xG-map.png}
    \caption{A visualisation of the generated $xG$ model. Darker areas signify that shots taken from that area have a higher $xG$ and thus more likely to lead to a goal.}
    \label{fig:xG-map}
\end{figure}

\subsubsection{Using the $xG$ model}
When it comes to using the $xG$ model rather than simply rewarding for the $xG$ of each shot action, we noticed during training that the model took a large number of 'bad' shots leading to repeated loss of possession. So we chose to 'punish' the reward by giving it a negative reward for each shot taken with a  $xG$ of below 0.2. This also ensured the model wouldn't try to 'game' the reward system by simply shooting each time it had the ball leading to a convergence of its policy to a nonsensical strategy.

\subsection{Pitch Control}

A Pitch Control model is used to calculate the probability that a team will be able to control or possess the ball at a given location of the football pitch at a given game state \cite{Spearman2016}. This could allow us to predict the success rate for a pass \cite{Higgins2023} with the following information: the position and velocity of each player, and the position and velocity of the ball. While it sounds simple, the information that could be extrapolated with this could be vast.

The addition of this metric to the reward shaper should improve the positioning of agents when they are not in control of the ball. This would reward the player for being in a strategic position which could lead to them receiving a pass allowing them to move further up the field. It is likely that this metric is going to be more useful in the multi-agent mode of the GFootball environment because in this case every player can be moved by the model simultaneously rather than the single-agent scenario, where only one can be moved at a certain moment. It is expected that in the former scenario, the model could learn to position each agent in a manner that optimises the pitch control metric rather than only positioning one agent.

\subsubsection{Calculating $PC$}
In real-world football analytics, $PC$ is calculated at a high resolution across the pitch to generate a map such as that in \ref{fig:pc-map}. The map is generated by dividing the pitch into 'cells', and calculating the probabilities of either team controlling the ball should it be in each cell and using a colour to represent the probability. In this case there are over 1500 cells to generate this visualisation. This would be computationally expensive and to do this at each step would be infeasible as it would cause training the model to be far too long. Thus we only calculate it in 24 target locations. This number was picked arbitrarily and could be adjusted depending on the time and computational resources available to anyone looking to replicate this study.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth, trim=1cm 2.5cm 1cm 2.5cm, clip]{pitch-control-map.png}
    \caption{A $PC$ map generated to study a pass from a real-world football match. The red and blue dots denote members of the red and blue teams. The red and blue space denotes the space 'controlled' by the red and blue teams respectively. White areas are those which is controlled by neither team.}
    \label{fig:pc-map}
\end{figure}

In one training episode there are 3001 steps, $PC$ depends on the position of the players, one would not expect a drastic change between each individual step thus we only calculate it in intervals of 5 steps.

When required, we process the information given by the environment which includes the positions and velocities of each player and the ball to calculate pitch control in the cells.  Rather than creating a bespoke pitch control calculation for this environment, it is far more efficient to use the one used in real-world analysis and adjust the parameters to fit the environment and needs of our reward shaper.
We convert the information given into 'Player' objects. Each 'Player' object has the following parameters: their maximum speed, reaction time, time to intercept sigma $\sigma_{TTI}$, attacking lambda $\lambda_A$, defending lambda $\lambda_D$, and their position.

The parameter $\sigma_{TTI}$ is used to adjust the sigmoid function which calculates the time to intercept \cite{Spearman2018}. This parameter was unadjusted. However $\lambda_A$ and $\lambda_B$ are used to give the attacking or defending a team a slight bias. This applies in real world situations where the defending team has a slight advantage in controlling the ball due to psychological and other miscellaneous factors. However in the environment these can be set to the same as these factors do not come into play. $\lambda_{GK}$ is set to triple the value of $\lambda_D$, this signifies that the keeper has a higher chance of controlling the ball owing to the fact that they can use their hands.

Other parameters such as player speed and reaction time can be set specifically for the player but again as we are operating this metric in an environment with all players being simulated, we can simply disregard these parameters.

We then finally calculate the probability of either team to control the ball at a target location. We first calculate the time for the ball to travel from its current location to the target location, this is done via simply dividing the distance which is calculated via the Cartesian distance formula by the average speed of the ball which is set to 15 \cite{Spearman2018}. We find the nearest attacking and defending player, if either one of them is able to reach the target significantly faster i.e 1 second before the opposition and the ball, we assign the probabilities of controlling the ball at that location as (1.0, 0.0) in favour of the 'nearer' team. This allows us to not perform a computationally expensive calculation at each cell.

If the above condition isn't met i.e it is not 'obvious' which team would control the ball, we then take into account all players close enough to contest the ball. These players would reach the target less than one second after the opposition and the ball, and thus be able to 'fight' for the ball. To obtain the probability of each player receiving the ball we solve the following differential equation \cite{Spearman2018} using an Euler's step iterative algorithm

\begin{equation}
    \frac{d P P C F_j}{d T}\left(t, \vec{r}, T \mid s, \lambda_j\right)=\left(1-\sum_k P P C F_k\left(t, \vec{r}, T \mid s, \lambda_j\right)\right) f_j(t, \vec{r}, T \mid s) \lambda_j
\end{equation}
Where $f_j(t, \overrightarrow{r, T} \mid s)$ represents the probability that player $j$ at time $t$ can reach location $r$ within some time T. We incorporate time of flight of the ball by setting $P P C F_j\left(t, \vec{r}, T \mid s, \lambda_j\right)=0$ when $T$ is less than the time of flight of the ball at location $r$. These flight times are discussed in more detail in Section 3.1.2. The parameter $\lambda_i$ is one of the aforementioned $\lambda_A$ and $\lambda_B$ (in this case, both are the same).

Summing over the probabilities $P P C F_j$ and $P P C F_k$ gained by solving the equation gives us the probabilities of the attacking or defending team gaining control of the ball. 

\subsubsection{Using the calculated $PC$}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\textwidth]{pc-diagram.png}
    \caption{Pictorial representation of the 24 locations where pitch control is calculated. Areas with the same letters indicate same importance given to the areas.}
    \label{fig:pc-diagram}
\end{figure}

It is more valuable to have control over certain areas of the pitch than other areas. To express this mathematically, we split the cells into 7 categories as seen in \ref{fig:pc-diagram} and awarded each one an arbitrary weighting. This is done via our intuition of which areas would be more vital to control in order to win the game.
The reward component for pitch control is thus calculated by:
\begin{equation}
    Reward_{PC} = \sum_{n=1}^{24}   PC_i * RF_i
\end{equation}
$PC_i$ denotes the pitch control in an area $i$, where $i$ is the number used to denote an area of the pitch with 1 being the top left corner, and $i$ increments from left to right, and top to bottom, and $RF_i$ denotes the reward factor for that area which is assigned according to the aforementioned categories.

\subsection{Expected Possession Value}


Expected Possession Value ($EPV$) is a measure of the likelihood of a team scoring or conceding the next goal at any time instance [11] such that  $EPV \in  [-1, 1] $. As expected, quantifying any given state in football into a single number, is quite complex. The ideal is to be able to do this with similar inputs as the aforementioned Pitch Control model, namely, the location and velocity of each player and the ball. A major hurdle in creating a precise $EPV$ model is the lack of labelled data. Recently many strides have been made in the accumulation of this information, but are primarily only held by football clubs or private organisations and are not easily available for research.

\subsubsection{Creating the $EPV$ Model}
A simpler $EPV$ model based on the position of the ball is possible however, and
has been made available by the team behind Soccermatics, a comprehensive course on using mathematical modelling techniques in the game of football. This $EPV$ model simply returns the expected chance of scoring or conceding a goal for the team currently holding the ball based on its position.

The available $EPV$ model still allows us to reward passes that bring the team “closer” to a goal as any pass that the agent creates that adds EPV would lead to a reward, and vice versa. This would stress the importance of forward passing, but also to areas where one is more likely to score, for example, it is preferential to have the ball in the centre of the pitch rather than the extreme left or right, as it would be easier to shoot and score.
The quantification of this human intuition is precisely why statistics such as these have become popular to analyse football games and scout talent and why this project aims to use the $EPV$ model in the shaped reward.

The model thus is a simple 2D array where each element stores the $EPV$ value for that part of the pitch,

\subsubsection{Using the $EPV$ model}
We store the $EPV$ model as a csv file and use the NumPy library to read it in. Every $k$ steps we check the location of the ball and reward the model with the $EPV \times RF_{EPV}$ where $RF_{EPV}$ is the factor used to scale the effect of the $EPV$ component. 

\subsection{Final reward}
To summarise, every step the agent is rewarded as follows:

\begin{multline}
    Reward = (Reward_{xG} \times RF_{xG}) + (Reward_{xT} \times RF_{xT}) \\
    + (Reward_{PC} \times RF_{PC}) + (Reward_{EPV} \times RF_{EPV})
\end{multline}

$Reward_{xG}$ is set to 0 unless the action taken by the agent is a shot in which case the $xG$ is calculated $Reward_{xT}$ is set to 0 unless a pass is received by a teammate and then the $xT$ is calculated. Every 5 frames, the $Reward_{PC}$ and $Reward_{EPV}$ are given else it is set to 0 to prevent the agent trying to 'game' this reward. The aim of the shaped reward is to 'guide' the agent to take a path that leads it to goal.All $RF$ are set arbitrarily via intuition and with the guiding principle that the cumulative additional reward given throughout an episode should be less than the reward given for a goal. The aim of the shaped reward is to 'guide' the agent to take a path that leads it to goal. This principle is implemented to avoid the agent(s) from prioritising passing or simply keeping the ball in areas with high $EPV$ rather than fulfilling the objective of scoring.

Given more time and computational resources, further study would be conducted which aimed to vary the reward factors allowing a study of which reward information or combination of reward information leads to the highest gain in performance.

\section{Reinforcement Learning Algorithms}

For the comparative study to be performed, aforementioned various Reinforcement Learning Algorithms are implemented. These algorithms are implemented solely as a way to test our reward shaper, we can consider them to be control variables for the experiment. These algorithms define the behaviour of the agent(s) as  seen in Figure \ref{fig:rl-frameworks}. The following sections discuss the implementation and choices we make for each algorithm.

Before a discussion of the algorithms we implemented, it is imperative to discuss the system used when training the agent using each algorithm.
An important part of this is the system these algorithms are run on. Since we use Docker to create a container within which all training is run allowing for the study to be potentially run on any system with Nvidia CUDA capabilities. Thus the available system is the main constraint on what can be run as the amount of studies that can be conducted is dependent on the computational resources available. The system has a Ryzen 5 6-core CPU and an Nvidia RTX 4070 (12G Memory). These are the main components which decide the duration of the training process.

\subsection{Proximal Policy Optimization}

In policy-based RL \cite{arulkumaran2017deep}, the goal is to learn to approximate the ideal policy ($ \pi^* $).
The policy is first parameterised such that we have $ \pi_\theta$ where $\theta$ are our parameters which defines the behaviour of $\pi$. $$ \pi_\theta(s)=\mathbb{P}[A \mid s ; \theta] $$ The policy, when given a state $s$, outputs a probability distribution $\mathbb{P}$, over the action space $A$ assuming that we are in the state $s$. We defined an objective function $J(\theta)$ which is the expected cumulative reward, and aim to find the $\theta$ which maximises this objective function. This can be done using optimisation functions such as hill-climbing and simulated annealing. To summarize, policy-based RL methods learn an optimal policy by trying various actions in various states and using its experience to learn how to maximize the reward gained from its actions in the long-term.

Policy gradient methods \cite{Peters2010} are a subclass of policy-based techniques that optimise parameterised policies with respect to the objective function using techniques such as gradient descent rather than hill-climbing. In 2017, Proximal Policy Optimization Algorithms (PPO) \cite{Schulman2017} were introduced with the aim to be simpler to implement, and reduce the number of data samples required to reach a certain level of learning performance. Since the GFootball environment was released in 2019, it follows that PPO algorithms have been the preferred method of many researchers (eg, \cite{Yu2022, Wen2022}). As a result this algorithm is implemented to test the effectiveness of the domain-specific reward function.

In PPO, the policy is typically represented by a parameterized function, often a neural network, which takes the current state as input and outputs a probability distribution over actions. The objective is to maximize the expected cumulative reward by adjusting the parameters of this policy function.

Rather than directly maximizing the expected reward, PPO optimizes a surrogate objective function that approximates the policy improvement. This surrogate objective incorporates a constraint to ensure that the updated policy does not deviate too far from the previous policy, which helps to stabilize the training process.

Mathematically, the objective function in PPO can be expressed as follows:

\[
\max_\theta \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}\left( r_t(\theta), 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \right]
\]

where:
\begin{itemize}

    \item  \( \theta \) represents the parameters of the policy,
    \item  \( r_t(\theta) \) is the ratio of probabilities of the new policy to the old policy,
    \item  \( \hat{A}_t \) is an estimate of the advantage function, representing the advantage of taking an action in a particular state over the average action value in that state,
    \item \( \epsilon \) is a hyperparameter controlling the size of the policy update.

\end{itemize}

The objective function encourages the policy to move in the direction that increases the probability of actions with higher advantages while limiting the size of the update to ensure stability.

To implement the PPO, we use the StableBaselines \cite{stable-baselines} package. This allows us to simply specify the hyperparameters and provide the environments and run our training. PPO can be run simultaneously, i.e rather than running one agent on one environment we can run multiple agents on multiple environments.

The primary challenge faced in the implementation of this algorithm was ensuring the system used the GPU rather than unsuitable alternatives. To remedy this, we use a GPU-enabled version of TensorFlow provided by Nvidia and a Nvidia docker runtime to ensure the dockerized container has access to the system's GPU. Without doing so, we lose immense performance and struggle to get 

The hyperparameters used in RL algorithms have wide-ranging effects on the behaviour and learning performance of the agents and many studies have and can be conducted on such.However this is not the focus of this study, thus the hyperparameters used in this implementation of the PPO and are the same used here \cite{Kurach2020} and the effects of each are not discussed in detail here.  is written by the team that built the GFootball environment and subsequently bench-marked it and thus it seems prudent to use their values as this is simply a control aspect to a comparative study about the effect of reward shaping.

\begin{table*}[h!]
\caption{Hyperparameters used in PPO.}
\begin{center}

\begin{tabular}{lrrr}
\toprule
\textbf{Parameter} & \textbf{Value} \\ \midrule
Action Repetitions & 1 \\
Clipping Range ($\epsilon$)  & .115\\
Discount Factor ($\gamma$) &  .997 \\
Entropy Coefficient & .00155\\
GAE ($\lambda$) & .95 \\
Gradient Norm Clipping & 0.115 \\
Learning Rate  & 1.1879e-4 \\
Number of Actors & 16 \\
Optimizer & Adam \\
Training Epochs per Update & 2 \\
Training Mini-batches per Update & 4 \\
Unroll Length/$n$-step & 512 \\
Value Function Coefficient & .5 \\
\bottomrule
\end{tabular}
\label{tab:ppo_hparams_values}
\end{center}
\end{table*}

\subsection{Deep Q Network}

In value-based methods \cite{arulkumaran2017deep}, rather than approximating the mapping of a state to an action, we aim to learn a value function with the goal of estimating the expected value of being at that state. However the goal of any RL agent is to have an optimal policy, how do we reach this via our value function? We use the function to estimate the value to be gained from undertaking each of the available action and undertake the action with the highest value. Thus it is simply a greedy policy, i.e one that aims to gain the highest value from the specified value function.

Rather than training the policy directly, finding an optimal value function ($Q^*(s,a)$), where the output of the function is the expected cumulative reward of taking action $a$ in state $s$. The optimal policy is then simply taking the action with the max output from the $Q^*$ function as expressed mathematically, $ \pi^*(s)=\arg \max _a Q^*(s, a)$ 

The most renown value-based method is the Deep Q Learning algorithm, developed by DeepMind in 2015 with the goal to solve a wide range of Atari games \cite{Mnih2013}. The ideal Q function would be a table containing values for each and every combination of a state and an action, but in more complex environments with a large (potentially even infinite) state space, this is impractical and infeasible. In the DeepMind paper the Deep-Q Network algorithm was introduced combining reinforcement learning and deep neural networks at scale. This was one of the biggest advancements in the field at the time, later resulting in the success of AlphaGo by DeepMind in 2017. As a result of its wide use in the field, Deep Q networks are implemented in this project to demonstrate the impact that the addition of domain-specific knowledge to the reward function can have on an RL agent in one of the most popular RL methods.

This is implemented again using the StableBaselines package for the aforementioned reasons. The hyperparameters used are again those used in benchmarking for the GFootball introductory paper.


\begin{table*} [h!]
\caption{Hyperparameters used in DQN}
\begin{center}
\begin{tabular}{lrrr}
\toprule
\textbf{Parameter} & \textbf{Value} \\ \midrule
Action Repetitions & 1\\
Batch Size & 512 \\
Discount Factor ($\gamma$) & $0.997$\\
Evaluation $\epsilon$ & .01 \\
Learning Rate & $1.1879e-4$ \\
Optimizer & Adam\\
Replay Priority Exponent & $\{0., .4, .5, .6, .7, .8\}$  \\
Target Network Update Period & 2500 \\
\bottomrule
\end{tabular}
\label{tab:dqn_hparams_values}
\end{center}
\end{table*}

\subsection{Value Decomposition Network}
As mentioned previously, value-based methods aim to estimate the value of being in a certain state rather than directly learning a policy. Value decomposition networks (VDNs) take this a step further by decomposing the value function into separate components, each representing the value of different aspects or dimensions of the state-action space. This decomposition allows the agent to better understand the underlying structure of the problem, potentially leading to more efficient learning and improved performance.

Rather than treating the value function as a single entity, VDNs break it down into components that correspond to different features or objectives within the environment. For example, in a game scenario, these components might represent the value of achieving certain goals, avoiding obstacles, or maximizing rewards.

By decomposing the value function, VDNs provide a more nuanced understanding of the state-action space, allowing the agents to make more informed decisions. Instead of simply selecting actions based on the overall value of a state, the agent can consider the contributions of each component to determine the best course of action.

One advantage of VDNs is their flexibility in handling complex environments with diverse objectives. By decomposing the value function, they can effectively capture the multidimensional nature of many real-world problems, leading to more robust and adaptable agents.

In summary, value decomposition networks offer a sophisticated approach to reinforcement learning by breaking down the value function into interpretable components. This enables agents to better understand and navigate complex environments, ultimately leading to more efficient learning and improved performance.

Considering Pitch Control is a component of our reward function, a potential outcome is that the agents tend towards optimal positioning. This could however require a large number of training episodes requiring either a lot of time and computational resources.

The implementation was done using the XuanCe \cite{liu2023xuance} and PyTorch \cite{paszke2019pytorch} packages. XuanCe is an ensemble of single-agent and multi-agent learning algorithms.

While the implementation of this algorithm is available in the research's repository, results from these are not available as of yet as a result of the lack of access to appropriately powerful computational resources that would be required for multi-agent training.

\subsection{Multi-Agent Proximal Policy Optimisation}

Multi-agent Proximal Policy Optimization (PPO) is a reinforcement learning algorithm designed to train policies for multiple agents simultaneously in a way that balances exploration and exploitation while ensuring stability during training. Similar to the previously mentioned single-agent PPO, the focus is on learning a policy directly rather than estimating a value function.


Overall, multi-agent PPO aims to find an optimal policy by directly parameterizing the policy function and optimizing it to maximize the expected cumulative reward while maintaining stability during training, similar to value-based methods but with a different approach to policy optimization.

Similar to the last algorithm, the implementation was done using the XuanCe \cite{liu2023xuance} and PyTorch \cite{paszke2019pytorch} packages.

Again, while the implementation of this algorithm is available in the research's repository, results from these are not available as of yet as a result of the lack of access to appropriately powerful computational resources that would be required for the training to be completed within a feasible time period.

\section{Experimentation}
The PPO and DQN algorithms were run twice with the same seed. This was done once with the 'scoring' reward, i.e +1 for a goal scored, -1 for a goal conceded, and once with the reward shaper discussed above. The primary method of analysis is going to be the learning curve. In the context of this paper and the field of RL, the learning curve is a representation of the performance of an RL agent over time. The performance of the RL agent is usually represented by the reward however since the reward shaper gives an additional reward this would be an unfair comparison. Thus, we use the goal difference in each episode as a measure of performance. Strictly speaking, since the reward shaper only values offensive actions, it would be more appropriate to use only goals scored as an evaluation, however this would be a less holistic view of football and thus goal difference is used. The hypothesis of this experiment is that the agents trained with the reward shaper would score more goals and have a higher goal difference as a result. Further analyses will be conducted using secondary statistics such as possession (i.e the percentage of time for which each team had the ball), number of shots, number of passes, and a measure of the correlation between the components of each reward cumulatively received during the episode and its goal difference. 

The analysis should not only provide us with an understanding of the impacts of the sparse and dense reward problem, but also the impact of certain statistics on football. The further analyses would give us an approximation of the usefulness of these statistics in real-world recruiting and tactical decisions. As of now, it would be the closest we can get to a quantitative analysis of the impact of these statistics in football tactics. To get closer, a more accurate simulation of football would be required and multiple iterations using a different combination of reward factors allowing us to zero in on the most optimal weighting to apply to each of these statistics.

\subsection{Analysis}
The first run of experimentation is the PPO training done with the scoring reward and the custom reward whose learning curves represented by the blue and orange graph lines respectively. The PPO algorithm runs 8 episodes concurrently, thus to have an accurate view of the learning curve, rather than a data point in the graph representing the goal difference of one episode, each data point represents the average of 8 concurrent episodes.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{PPO-reward-experiment.png}
    \caption{A graph representing the learning curve of the PPO training runs}
    \label{fig:PPO-exp-gd}
\end{figure}

You can notice that both graphs appear to plateau after a certain number of episodes. This is typical of RL algorithms, this occurs when the agent has learned as much as it can from the environment given its current learning algorithm (along with the effects of the various hyperparameters) and the policy representation. The reward shaper is an adaptation of the environment and thus has a clear effect on the learning curve represented by orange.

The reason for the graphs roughness rather than a smooth ascent followed by the plateau is the tendency of RL agents to explore the environment, trying different methods to gain a reward rather than simply exploiting known methods. This allows the agent to gain further insights into the environment and discover new tactics. This tendency is usually defined by a hyperparameter, in the case of PPO the hyperparameter is the entropy coefficient.

The most prominent feature of the graph is that the learning curve with the reward shaper (Run B) is clearly higher than that with the 'scoring' reward (Run A). This is incredibly promising as it is an indicator of the hypothesis being correct. In the initial stages, Run A performs better but is then outperformed by Run B, a potential reason for this is because Run B initially focuses on the short-term rewards given by the reward shaper and thus doesn't explore further until after a certain number of episodes after which Run A clearly outperforms Run B.

An initial iteration of the experiment showed almost no difference between either run. Further investigation into this outcome showed that the agent came to 'discover' that shooting the ball would give it an immediate, if small, reward and thus took a shot at every opportunity. This required a change such that shots that were 'bad' i.e less than a certain $xG$ value (0.2 in this study) would be penalised. This prevents the agent from gaming the system by taking shots that have no benefit and only shoot when there is a realistic chance of scoring. This phenomenon also indicates that a large weakness of the agent trained with the 'scoring' reward learns that to score a goal it needs to take the 'shot' action. Rather than coming to understand the idea of reaching a suitable position before taking the 'shot' action at each possible moment. The 'punishment' for bad shots seems to have been essential as it prompts the agent to explore other paths to goal which are also rewarded by the $xT$ and $EPV$ reward components.

\begin{figure}[!h]
    \centering
    \hspace*{-2cm}
    \includegraphics[width=1.25\textwidth]{CustomStats-CRM.png}
    \caption{A matrix showing the Pearson correlation between various statistics collected from training episodes}
    \label{fig:CRM-Custom}
\end{figure}

\pagebreak
\bibliographystyle{abbrv}
\bibliography{DissertationPaperRepo}

\end{document}
